{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextBlob Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXygNcpEGCBPbRai7UEHPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrikanthGuggila/TextBlob-Tutorial/blob/main/TextBlob_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK30EZzJJPih"
      },
      "source": [
        "**TextBlob**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNCll_vrDw7S"
      },
      "source": [
        "TextBlob is library use in Natural Language Processing. Using TextBlob, we can perfor NLP tasks such as Tokenization, Lemmatization, Part-of-Speech tagging, noun-phrase extraction, sentiment analysis and many more.\n",
        "\n",
        "TextBlob is built on top of NLTK library\n",
        "\n",
        "TextBlob stands on the giant shoulders of NLTK and pattern, and plays nicely with both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1P1s8n2J2y4"
      },
      "source": [
        "**Features of TextBlob**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amULn8jzJ6Q7"
      },
      "source": [
        "* Noun phrase extraction\n",
        "* Part-of-speech tagging\n",
        "* Sentiment analysis\n",
        "* Classification (Naive Bayes, Decision Tree)\n",
        "* Tokenization (splitting text into words and sentences)\n",
        "* Word and phrase frequencies\n",
        "* Parsing\n",
        "* n-grams\n",
        "* Word inflection (pluralization and singularization) and lemmatization\n",
        "* Spelling correction\n",
        "* Add new models or languages through extensions\n",
        "* WordNet integration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdDKbr1YJTos"
      },
      "source": [
        "**Install TextBlob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYHsbdpxKm7c",
        "outputId": "f6a9f3fc-b2ef-44b9-f60a-faafa6a070e4"
      },
      "source": [
        "!pip install textblob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqxXygzxLHaK"
      },
      "source": [
        "**Installing/Upgrading From the PyPI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kdRMVSkLLd1"
      },
      "source": [
        "$ pip install -U textblob\n",
        "$ python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaMvQ9Z6LOug"
      },
      "source": [
        "This will install TextBlob and download the necessary NLTK corpora. If you need to change the default download directory set the NLTK_DATA environment variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4smvuvRTLRDY"
      },
      "source": [
        "**Downloading the minimum corpora**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXcB_-I_LThy"
      },
      "source": [
        "If you only intend to use TextBlob’s default models (no model overrides), you can pass the lite argument. This downloads only those corpora needed for basic functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rVCudDVLVPE"
      },
      "source": [
        "$ python -m textblob.download_corpora lite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWJaXVhHLX2i"
      },
      "source": [
        "**Install with conda**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rxNXKm2LXZI"
      },
      "source": [
        "$ conda install -c conda-forge textblob\n",
        "$ python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rymtckkFLxbP"
      },
      "source": [
        "**From Source**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZEXR_WL6aq"
      },
      "source": [
        "You can clone the public repo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSODQ1qeMAAO"
      },
      "source": [
        "https://github.com/sloria/TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekcn56rJLxDK"
      },
      "source": [
        "$ git clone https://github.com/sloria/TextBlob.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOtAACqyMEbY"
      },
      "source": [
        "Once you have the source, you can install it into your site-packages with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M47ouOHqDr4t"
      },
      "source": [
        "$ python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tgmhhXAMG9w"
      },
      "source": [
        "**Get the bleeding edge version**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwJLpFwFMOXA"
      },
      "source": [
        "To get the latest development version of TextBlob, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYaCF5BhMKBV"
      },
      "source": [
        "$ pip install -U git+https://github.com/sloria/TextBlob.git@dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHJbZ0Y9MSvB"
      },
      "source": [
        "**Migrating from older versions (<=0.7.1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEZNFvuAMWSP"
      },
      "source": [
        "As of TextBlob 0.8.0, TextBlob’s core package was renamed to textblob, whereas earlier versions used a package called text. Therefore, migrating to newer versions should be as simple as rewriting your imports, like so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntarPb5MYnc"
      },
      "source": [
        "New:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPkE-BvjMYOw"
      },
      "source": [
        "from textblob import TextBlob, Word, Blobber\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob.taggers import NLTKTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxal0hjCMbnF"
      },
      "source": [
        "Old:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV7TooAkMW1H"
      },
      "source": [
        "from text.blob import TextBlob, Word, Blobber\n",
        "from text.classifiers import NaiveBayesClassifier\n",
        "from text.taggers import NLTKTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uADRjqnCMgSy"
      },
      "source": [
        "**Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jokYgDfLMih7"
      },
      "source": [
        "TextBlob supports Python >=2.7 or >=3.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyZN8qGdMkAB"
      },
      "source": [
        "**Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H__exh5nMmXj"
      },
      "source": [
        "TextBlob depends on NLTK 3. NLTK will be installed automatically when you run pip install textblob or python setup.py install.\n",
        "\n",
        "Some features, such as the maximum entropy classifier, require numpy, but it is not required for basic usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PazIHdf3M2CI"
      },
      "source": [
        "**Create a TextBlob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TEnlMbYM6i2"
      },
      "source": [
        "from textblob import TextBlob\n",
        "text_blob = TextBlob('I am learning Natural Language Processing with Python, Learning Python also very easy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZVldHvS1su3",
        "outputId": "dd84fb63-d5ac-49d4-84b2-ba16e1a80cad"
      },
      "source": [
        "text_blob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"I am learning Natural Language Processing with Python, Learning Python also very easy\")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa3yKQrI1wx9"
      },
      "source": [
        "**Part-of-speech Tagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdA6x9yKreJz"
      },
      "source": [
        "NLTK is a library which is also used for text processing and text analysis\n",
        "TextBlob is bulit upon NLTK library.\n",
        "\n",
        "To get the tags, we need to download the 'punkt' and 'averaged_perceptron_tagger' from nltk library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hphbPi66rdUE",
        "outputId": "6c3d4859-0a21-4e5a-cdd3-f55d68d43e66"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHHAt6OMrPzr",
        "outputId": "f737b2e0-70e3-4caf-accf-7f456f1310c1"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGywW7pT1uGM",
        "outputId": "da62c91d-fa1c-41bf-8c61-c0a57eaaaa20"
      },
      "source": [
        "text_blob.tags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('learning', 'VBG'),\n",
              " ('Natural', 'NNP'),\n",
              " ('Language', 'NNP'),\n",
              " ('Processing', 'VBG'),\n",
              " ('with', 'IN'),\n",
              " ('Python', 'NNP'),\n",
              " ('Learning', 'NNP'),\n",
              " ('Python', 'NNP'),\n",
              " ('also', 'RB'),\n",
              " ('very', 'RB'),\n",
              " ('easy', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJqYW-gXkZI"
      },
      "source": [
        "**POS tagging Advanced**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNs68g3cXqql"
      },
      "source": [
        "TextBlob currently has two POS tagger implementations, located in textblob.taggers. The default is the PatternTagger which uses the same implementation as the pattern library.\n",
        "\n",
        "The second implementation is NLTKTagger which uses NLTK’s TreeBank tagger. Numpy is required to use the NLTKTagger.\n",
        "\n",
        "Similar to the tokenizers and noun phrase chunkers, you can explicitly specify which POS tagger to use by passing a tagger instance to the constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMt1Lvo0YTzV",
        "outputId": "5339197d-2c8c-4690-95a9-f3f343cbf563"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8maPbjFpXoUI",
        "outputId": "54122fc0-d5ca-4821-a877-ec0867abe15d"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from textblob.taggers import NLTKTagger\n",
        "tagger = NLTKTagger()\n",
        "blob = TextBlob(\"we are learning parts of speech taggers in textblob\", pos_tagger=tagger)\n",
        "blob.pos_tags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('learning', 'VBG'),\n",
              " ('parts', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('speech', 'NN'),\n",
              " ('taggers', 'NNS'),\n",
              " ('in', 'IN'),\n",
              " ('textblob', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-molEzsECX"
      },
      "source": [
        "**Noun Phrase Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khEaFYQfsOUH",
        "outputId": "819946ec-055d-4fcc-f1d2-c84e0a383c54"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxCnJz4A11R8",
        "outputId": "0bad71de-a06a-420a-fd89-9177340c6fe3"
      },
      "source": [
        "text_blob.noun_phrases"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['language processing', 'python', 'learning python'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkib3ThlVPTW"
      },
      "source": [
        "**Noun Phrase Extraction Advanced**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjfYnOQOVop1"
      },
      "source": [
        "TextBlob currently has two noun phrases chunker implementations, textblob.np_extractors.FastNPExtractor (default, based on Shlomi Babluki’s implementation from this blog post) and textblob.np_extractors.ConllExtractor, which uses the CoNLL 2000 corpus to train a tagger.\n",
        "\n",
        "You can change the chunker implementation (or even use your own) by explicitly passing an instance of a noun phrase extractor to a TextBlob’s constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVE4lNVhXDCH",
        "outputId": "4074e823-22c3-423c-85c9-685eda5eb47e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ycA4e9QXLHc",
        "outputId": "11fe2fba-0da4-4879-dd49-d82ab601a0f8"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtDQBo0VXVq_",
        "outputId": "37999903-eb83-45d8-b284-9a1dd8d54f6a"
      },
      "source": [
        "nltk.download('conll2000')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5NXTUVZVZzr",
        "outputId": "eb2cc098-d879-4dec-caa3-2c518f1dc5b8"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from textblob.np_extractors import FastNPExtractor\n",
        "from textblob.np_extractors import ConllExtractor\n",
        "fast_np_extract = FastNPExtractor()\n",
        "blob1 = TextBlob(\"Python is High level programming language and easy to learn\", np_extractor=fast_np_extract)\n",
        "print(blob1.noun_phrases)\n",
        "coll_extract = ConllExtractor()\n",
        "blob2 = TextBlob(\"Python is High level programming language and easy to learn\", np_extractor=coll_extract)\n",
        "print(blob2.noun_phrases) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['python', 'high level']\n",
            "['python', 'high level programming language']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi98WMRosrWL"
      },
      "source": [
        "**Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk2w-ho8x5gM"
      },
      "source": [
        "Sentiment function returns two properties\n",
        "1. polarity \n",
        "2. subjectivity\n",
        "\n",
        "* polarity lies between -1 and 1, -1 represents it is negative sentiment where 1 represents it is a positive sentiment\n",
        "\n",
        "* subjectivity lies between 0 and 1, 0 reprensents it is a factual information where as 1 represents it is a personal opinion "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzqfKF7QsL9x",
        "outputId": "5ebef744-3c59-47a8-a7c9-200362cca15f"
      },
      "source": [
        "statement = TextBlob('Learning python is super fun and easy')\n",
        "statement.sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.35555555555555557, subjectivity=0.5666666666666668)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRxkHrY6s9eO",
        "outputId": "3f73937d-f7ab-4c40-c828-8aa6f2d0df67"
      },
      "source": [
        "statement.sentiment.polarity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35555555555555557"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5TYjXzxyqIW",
        "outputId": "98b3e1fa-8d5a-4ca6-d1f9-ca9b31156abc"
      },
      "source": [
        "statement.sentiment.subjectivity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5666666666666668"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIp718tJysof",
        "outputId": "294f491c-462a-4e7a-cb09-9825c5184de1"
      },
      "source": [
        "statement2 = TextBlob('regular less sleep is not good for health')\n",
        "print('Polairity: ', statement2.sentiment.polarity)\n",
        "print('Subjectivity: ', statement2.sentiment.subjectivity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Polairity:  -0.1722222222222222\n",
            "Subjectivity:  0.24786324786324787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gxhP2igz6Xu"
      },
      "source": [
        "* The polarity is close to -1, we can say it is a negative sentiment\n",
        "* The subjectivity is close to 0, we can say it is a fact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxTsV1_h08FO"
      },
      "source": [
        "**Advanced Sentiment Anayzers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANA5XAkUFLXq"
      },
      "source": [
        "TextBlob allows you to specify which algorithms you want to use under the hood of its simple API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTFkrAORFMWe"
      },
      "source": [
        "The textblob.sentiments module contains two sentiment analysis implementations, PatternAnalyzer (based on the pattern library) and NaiveBayesAnalyzer (an NLTK classifier trained on a movie reviews corpus).\n",
        "\n",
        "The default implementation is PatternAnalyzer, but you can override the analyzer by passing another implementation into a TextBlob’s constructor.\n",
        "\n",
        "For instance, the NaiveBayesAnalyzer returns its result as a namedtuple of the form: Sentiment(classification, p_pos, p_neg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAd00c5L1A_f",
        "outputId": "709d4f33-1e60-483e-b077-84b9278927d7"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "blob = TextBlob('I love Learning TextBlob')\n",
        "blob.sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.5, subjectivity=0.6)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eHL99p9GHbK"
      },
      "source": [
        "As we are trying to use NaiveBayesAnalyzer classification model which is aleady used on movie reviews data set, we need to download movie_reviews dataset from nltk library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6UbeRt_GeAd",
        "outputId": "f54fdb0b-3815-440e-d669-c418b2750478"
      },
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVWNCoAxF67p",
        "outputId": "8ef5169a-dfe3-4078-b79d-333ec7a2b0ca"
      },
      "source": [
        "blob = TextBlob('I love Learning TextBlob', analyzer = NaiveBayesAnalyzer())\n",
        "blob.sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(classification='pos', p_pos=0.7085840931204725, p_neg=0.2914159068795271)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJjTtrK_HCwB",
        "outputId": "a1382b3f-e025-406b-8b7f-7bb1fadd2b1a"
      },
      "source": [
        "blob = TextBlob(\"I'm not interested in movies\", analyzer=NaiveBayesAnalyzer())\n",
        "print(blob.sentiment.p_pos)\n",
        "print(blob.sentiment.p_neg)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4747641843877517\n",
            "0.5252358156122483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQYrBhzh0ZAu"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJvXh2j1zHP0"
      },
      "source": [
        "text = TextBlob(\"Beautiful is better than ugly. \"\n",
        "                \"Explicit is better than implicit. \"\n",
        "                \"Simple is better than complex.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs8ocrLjJPQL",
        "outputId": "47fba8f7-b458-4272-b535-e7ad82d32142"
      },
      "source": [
        "text.words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Beautiful', 'is', 'better', 'than', 'ugly', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgMMzkFbJQLa",
        "outputId": "ed7a8e92-b094-4144-c5ec-5b9521b4c72b"
      },
      "source": [
        "for word in text.words:\n",
        "  print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beautiful\n",
            "is\n",
            "better\n",
            "than\n",
            "ugly\n",
            "Explicit\n",
            "is\n",
            "better\n",
            "than\n",
            "implicit\n",
            "Simple\n",
            "is\n",
            "better\n",
            "than\n",
            "complex\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4FJO2NeJYYZ",
        "outputId": "c93f29fb-adf8-4a2d-d0ca-aff1350d7137"
      },
      "source": [
        "text.sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence(\"Beautiful is better than ugly.\"),\n",
              " Sentence(\"Explicit is better than implicit.\"),\n",
              " Sentence(\"Simple is better than complex.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cc-sUXnPN2X",
        "outputId": "ccfd03c9-ccff-4935-ad4c-497c95ea20a9"
      },
      "source": [
        "for sentence in text.sentences:\n",
        "  print(sentence.sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment(polarity=0.2166666666666667, subjectivity=0.8333333333333334)\n",
            "Sentiment(polarity=0.5, subjectivity=0.5)\n",
            "Sentiment(polarity=0.06666666666666667, subjectivity=0.41904761904761906)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCLKvtvW2r9b"
      },
      "source": [
        "for sentence in text.sentences:\n",
        "  print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqi8il3pPei3"
      },
      "source": [
        "**Advanced Tokenizers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1SPHt7uRLmW"
      },
      "source": [
        "The words and sentences properties are helpers that use the \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "textblob.tokenizers.WordTokenizer and\n",
        "textblob.tokenizers.SentenceTokenizer classes, respectively.\n",
        "```\n",
        "\n",
        "\n",
        "You can use other tokenizers, such as those provided by NLTK, by passing them into the TextBlob constructor then accessing the tokens property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mADOBTIHPX7f",
        "outputId": "1c829687-c425-49ca-d05a-2522ef51dea4"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from nltk.tokenize import TabTokenizer\n",
        "token = TabTokenizer()\n",
        "blob = TextBlob(\"Beautiful\\tis\\tbetter\\tthan\\tugly.\", tokenizer=token)\n",
        "blob.tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Beautiful', 'is', 'better', 'than', 'ugly.'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4GrxiOG2by5",
        "outputId": "a364ad8c-3e73-41f5-ca68-bad51364a50e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXlpfZXv1lRb",
        "outputId": "d84acf42-acb1-46f7-d141-69ad1b47e4a1"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from nltk.tokenize import BlanklineTokenizer\n",
        "toekenizer = BlanklineTokenizer()\n",
        "blob = TextBlob(\"Beautiful\\nis\\nbetter\\nthan\\nugly.\")\n",
        "blob.tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Beautiful', 'is', 'better', 'than', 'ugly', '.'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qdFlfSAtt7l"
      },
      "source": [
        "**Blobber: A TextBlob Factory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJnjm9mCtx8U"
      },
      "source": [
        "It can be tedious to repeatedly pass taggers, NP extractors, sentiment analyzers, classifiers, and tokenizers to multiple TextBlobs. To keep your code DRY, you can use the Blobber class to create TextBlobs that share the same models.\n",
        "\n",
        "First, instantiate a Blobber with the tagger, NP extractor, sentiment analyzer, classifier, and/or tokenizer of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3ifn9Cv2NRG"
      },
      "source": [
        "from textblob import Blobber\n",
        "from textblob.taggers import NLTKTagger\n",
        "tb = Blobber(pos_tagger=NLTKTagger())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8HpimHDuVVd",
        "outputId": "76cd0d8a-708d-4d1f-a462-cfe933ddb69b"
      },
      "source": [
        "blob1 = tb(\"this is a blob\")\n",
        "blob2 = tb(\"this is another blob\")\n",
        "blob1.pos_tagger is blob2.pos_tagger"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZnssuD5wvut"
      },
      "source": [
        "**Word Inflection and Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyP-9JeKvEfS",
        "outputId": "56cacea5-bd0f-406f-bdf5-f566384d3325"
      },
      "source": [
        "text = TextBlob(\"Jeo Biden is elected as president of US elections in 2020\")\n",
        "text.words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Jeo', 'Biden', 'is', 'elected', 'as', 'president', 'of', 'US', 'elections', 'in', '2020'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z0sokL4JxN9P",
        "outputId": "7c61566a-7c24-493f-d35d-be5ebecdfd64"
      },
      "source": [
        "text.words[8].singularize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'election'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xN5MQyQexWJd",
        "outputId": "5580a09d-dc16-4f8d-c021-d7dd2ea1d16f"
      },
      "source": [
        "text.words[5].pluralize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'presidents'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y4UeMSWyKvd",
        "outputId": "89cc4cb2-768c-4c30-e2f3-644603ce641c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g7jwNHg-xhBC",
        "outputId": "da7663b8-28ae-4dd0-a1fc-363d922c99e0"
      },
      "source": [
        "from textblob import Word\n",
        "w = Word('octopi')\n",
        "w.lemmatize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'octopus'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y_YwjkMyBRO"
      },
      "source": [
        "speech_text = \"\"\"Wednesday night, approaching his 100th day in office, \n",
        "                  President Joe Biden addresses the joint session of Congress for the first time. \n",
        "                  The full text of his prepared speech below was released by the White House.\n",
        "                  Madame Speaker. Madame Vice President. \n",
        "                  No president has ever said those words from this podium, and it’s about time. The First Lady. \n",
        "                  The Second Gentleman. Mr. Chief Justice. \n",
        "                  Members of the United States Congress and the Cabinet – and distinguished guests.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwRS4wJ1zJE6",
        "outputId": "958564eb-2e92-42f7-d54d-9a7f4c984c1a"
      },
      "source": [
        "blob = TextBlob(speech_text)\n",
        "lemma_words = [word.lemmatize('v') for word in blob.words]\n",
        "lemma_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wednesday',\n",
              " 'night',\n",
              " 'approach',\n",
              " 'his',\n",
              " '100th',\n",
              " 'day',\n",
              " 'in',\n",
              " 'office',\n",
              " 'President',\n",
              " 'Joe',\n",
              " 'Biden',\n",
              " 'address',\n",
              " 'the',\n",
              " 'joint',\n",
              " 'session',\n",
              " 'of',\n",
              " 'Congress',\n",
              " 'for',\n",
              " 'the',\n",
              " 'first',\n",
              " 'time',\n",
              " 'The',\n",
              " 'full',\n",
              " 'text',\n",
              " 'of',\n",
              " 'his',\n",
              " 'prepare',\n",
              " 'speech',\n",
              " 'below',\n",
              " 'be',\n",
              " 'release',\n",
              " 'by',\n",
              " 'the',\n",
              " 'White',\n",
              " 'House',\n",
              " 'Madame',\n",
              " 'Speaker',\n",
              " 'Madame',\n",
              " 'Vice',\n",
              " 'President',\n",
              " 'No',\n",
              " 'president',\n",
              " 'have',\n",
              " 'ever',\n",
              " 'say',\n",
              " 'those',\n",
              " 'word',\n",
              " 'from',\n",
              " 'this',\n",
              " 'podium',\n",
              " 'and',\n",
              " 'it',\n",
              " '’',\n",
              " 's',\n",
              " 'about',\n",
              " 'time',\n",
              " 'The',\n",
              " 'First',\n",
              " 'Lady',\n",
              " 'The',\n",
              " 'Second',\n",
              " 'Gentleman',\n",
              " 'Mr',\n",
              " 'Chief',\n",
              " 'Justice',\n",
              " 'Members',\n",
              " 'of',\n",
              " 'the',\n",
              " 'United',\n",
              " 'States',\n",
              " 'Congress',\n",
              " 'and',\n",
              " 'the',\n",
              " 'Cabinet',\n",
              " '–',\n",
              " 'and',\n",
              " 'distinguish',\n",
              " 'guests']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjzyU0MU0JHs"
      },
      "source": [
        "**WordNet Integration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfTSseH3ztvw",
        "outputId": "3af5b75c-c586-40df-aed1-6d61c5458972"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from textblob.wordnet import VERB\n",
        "word = Word('octopus')\n",
        "word.synsets\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('octopus.n.01'), Synset('octopus.n.02')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJOBcf7T6qnT",
        "outputId": "a0de567c-af50-4a77-c76b-b010e7971245"
      },
      "source": [
        "Word('hack').get_synsets(pos=VERB)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('chop.v.05'),\n",
              " Synset('hack.v.02'),\n",
              " Synset('hack.v.03'),\n",
              " Synset('hack.v.04'),\n",
              " Synset('hack.v.05'),\n",
              " Synset('hack.v.06'),\n",
              " Synset('hack.v.07'),\n",
              " Synset('hack.v.08')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpwR0HCM60O6",
        "outputId": "27819dde-70c1-40ca-96da-0e8e625ef371"
      },
      "source": [
        "Word(\"octopus\").definitions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tentacles of octopus prepared as food',\n",
              " 'bottom-living cephalopod having a soft oval body with eight long tentacles']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An0fbnvR7FCL",
        "outputId": "88498b8c-d206-4676-be86-b9a010bcd783"
      },
      "source": [
        "from textblob.wordnet import Synset\n",
        "octopus = Synset('octopus.n.02')\n",
        "shrimp = Synset('shrimp.n.03')\n",
        "octopus.path_similarity(shrimp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjRNAFRM7rAE"
      },
      "source": [
        "**WordLists**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC7T_jv1w5BT",
        "outputId": "2ef59aa6-9c27-40e8-e1b4-bea514d691a8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R58RA3D7luY",
        "outputId": "7964985f-a21e-4b76-da39-ac562ef43557"
      },
      "source": [
        "animals = TextBlob(\"cat dog octopus\")\n",
        "animals.words\n",
        "animals.words.pluralize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['cats', 'dogs', 'octopodes'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H0S24rcwp22",
        "outputId": "35a71e32-51db-49c3-b12f-beb63a2504bd"
      },
      "source": [
        "animals = TextBlob(\"cats dogs octopodes\")\n",
        "animals.words\n",
        "animals.words.singularize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['cat', 'dog', 'octopus'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmH-zbnz75O9"
      },
      "source": [
        "**Spelling Correction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5pBYPtb70sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a41dfc7-28db-4b38-d664-e452ca5cbdef"
      },
      "source": [
        "from textblob import TextBlob\n",
        "b = TextBlob(\"I am goinf to learb nayural procescing langyage\")\n",
        "print(b.correct())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am going to learn natural processing language\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ZTvx4Ywdqf",
        "outputId": "9170b178-d8cd-4cb0-dce0-e73599723e06"
      },
      "source": [
        "from textblob import Word\n",
        "w = Word('favorate')\n",
        "w.spellcheck()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('favorite', 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucy1jDsu0VaT"
      },
      "source": [
        "**Word and Noun Phrase frequencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AkCFseBxYnn",
        "outputId": "9be4a555-a64e-4472-e631-12837600e6d5"
      },
      "source": [
        "text = TextBlob(\"Indian population is Very very very high compared to other countries population\")\n",
        "print('Population:',text.word_counts['population'])\n",
        "print('very:',text.word_counts['very'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Population: 2\n",
            "very: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pms7BuFc0u2N",
        "outputId": "83fc90a2-1df4-414c-91a3-f5a3533e8587"
      },
      "source": [
        "print('very:',text.words.count('very'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "very: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIl5dH2B2zGX",
        "outputId": "c5e35ba3-ea55-45a7-c112-376cbef00cfa"
      },
      "source": [
        "print('very:',text.words.count('very', case_sensitive=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "very: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndBMwQ0N3GzZ"
      },
      "source": [
        "**Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoBafwI8275J",
        "outputId": "ad6937d1-fa73-43db-df29-700ce7f66615"
      },
      "source": [
        "b = TextBlob(\"Now we are going to learn parsing\")\n",
        "print(b.parse())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now/RB/B-ADVP/O we/PRP/B-NP/O are/VBP/B-VP/O going/VBG/I-VP/O to/TO/B-PP/O learn/VB/B-VP/O parsing/VBG/I-VP/O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0D2N3K671k0"
      },
      "source": [
        "**n-grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaFAc55w3XWP",
        "outputId": "a81d9133-bd30-4fb1-add8-d5f5a4595423"
      },
      "source": [
        "blob = TextBlob(\"Now is better than ever\")\n",
        "blob.ngrams(n=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['Now', 'is']),\n",
              " WordList(['is', 'better']),\n",
              " WordList(['better', 'than']),\n",
              " WordList(['than', 'ever'])]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0_Nf0Ir7-5x",
        "outputId": "8694ee97-f28d-4f2f-9e1e-3cd86dab6ee8"
      },
      "source": [
        "blob = TextBlob(\"every in the IT industry trying to upskill themselves\")\n",
        "blob.ngrams(n=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['every', 'in', 'the']),\n",
              " WordList(['in', 'the', 'IT']),\n",
              " WordList(['the', 'IT', 'industry']),\n",
              " WordList(['IT', 'industry', 'trying']),\n",
              " WordList(['industry', 'trying', 'to']),\n",
              " WordList(['trying', 'to', 'upskill']),\n",
              " WordList(['to', 'upskill', 'themselves'])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jsFpZyD8R1W"
      },
      "source": [
        "**Text Classification System with TextBlob**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH_p6qGK8ZYi"
      },
      "source": [
        "***Spam Classifier with TextBlob***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "GxqaNhie8KcP",
        "outputId": "efb134a4-cb7e-4a88-c4a9-059bf6a0de26"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/SPAM text message 20170820 - Data.csv')\n",
        "df.head(10)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ham</td>\n",
              "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>spam</td>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category                                            Message\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
              "6      ham  Even my brother is not like to speak with me. ...\n",
              "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
              "8     spam  WINNER!! As a valued network customer you have...\n",
              "9     spam  Had your mobile 11 months or more? U R entitle..."
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1vdSZCTQzpy"
      },
      "source": [
        "data = zip(df.Message, df.Category)\n",
        "data = list(data)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNh0o_y_RJmi",
        "outputId": "277fcb62-ee65-4b78-ff84-ffe2182bf2a7"
      },
      "source": [
        "data[:10]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
              "  'ham'),\n",
              " ('Ok lar... Joking wif u oni...', 'ham'),\n",
              " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
              "  'spam'),\n",
              " ('U dun say so early hor... U c already then say...', 'ham'),\n",
              " (\"Nah I don't think he goes to usf, he lives around here though\", 'ham'),\n",
              " (\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\",\n",
              "  'spam'),\n",
              " ('Even my brother is not like to speak with me. They treat me like aids patent.',\n",
              "  'ham'),\n",
              " (\"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
              "  'ham'),\n",
              " ('WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.',\n",
              "  'spam'),\n",
              " ('Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030',\n",
              "  'spam')]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbsUzxKoSb8g",
        "outputId": "3d7ae683-68e7-4855-869f-ec6325870d2b"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5572"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQalWiwUWEhS",
        "outputId": "5709187b-568e-4744-c8a7-4d4073da552f"
      },
      "source": [
        "term = round(len(data)*0.8)\n",
        "term"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4458"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CADhwEoSWZ0T",
        "outputId": "1798591a-8d08-4e5b-a3b2-c636649ff791"
      },
      "source": [
        "train = data[:term]\n",
        "test = data[term:]\n",
        "print('Train data Size:',len(train))\n",
        "print('Test data Size:',len(test))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data Size: 4458\n",
            "Test data Size: 1114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tYuslsRW-mm"
      },
      "source": [
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier(train)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UqrquWviXeBX",
        "outputId": "2ec81fd0-f494-4084-db1e-2d07f45fd774"
      },
      "source": [
        "classifier.classify(data[term+1][0])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ham'"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIuXj53tXYA8",
        "outputId": "75ec2b0a-1dc7-44c6-8874-c046a4139ec4"
      },
      "source": [
        "data[term+1]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Die... I accidentally deleted e msg i suppose 2 put in e sim archive. Haiz... I so sad...',\n",
              " 'ham')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1eYjgnq6Y3P_",
        "outputId": "80f2430b-6223-40cc-b76e-5697491760b5"
      },
      "source": [
        "from textblob import TextBlob\n",
        "blob = TextBlob(\"The beer is good. But the hangover is horrible.\", classifier = classifier)\n",
        "blob.classify()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ham'"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KCWWn7dWZQZY",
        "outputId": "f2d4137a-8584-436f-fd6e-c7d817b3d70d"
      },
      "source": [
        "from textblob import TextBlob\n",
        "blob = TextBlob(\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\", classifier = classifier)\n",
        "blob.classify()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MuVMHsr2ZkQv",
        "outputId": "61d93787-0df9-4ac3-8a9d-303567cebbd6"
      },
      "source": [
        "classifier.classify(data[term+2][0])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UARy8048Z18a",
        "outputId": "2d20ccd4-5b64-409e-daef-74f574febc7c"
      },
      "source": [
        "classifier.accuracy(test)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9829443447037702"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt2lEqYOZ7ks",
        "outputId": "9507ac35-18de-488e-e2d0-893284921f8b"
      },
      "source": [
        "classifier.show_informative_features(10)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "          contains(STOP) = True             spam : ham    =    177.0 : 1.0\n",
            "            contains(16) = True             spam : ham    =    177.0 : 1.0\n",
            "           contains(Txt) = True             spam : ham    =     90.3 : 1.0\n",
            "        contains(Orange) = True             spam : ham    =     87.4 : 1.0\n",
            "        contains(Mobile) = True             spam : ham    =     83.2 : 1.0\n",
            "            contains(To) = True             spam : ham    =     83.2 : 1.0\n",
            "           contains(txt) = True             spam : ham    =     80.3 : 1.0\n",
            "        contains(camera) = True             spam : ham    =     66.1 : 1.0\n",
            "         contains(award) = True             spam : ham    =     66.1 : 1.0\n",
            "        contains(pounds) = True             spam : ham    =     61.8 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gMc51MgaPi8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}